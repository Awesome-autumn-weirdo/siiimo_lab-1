# Лабораторная работа №1  
## Построение и оценка регрессионных моделей


Цель работы: изучение основ линейной регрессии, построение простейших моделей регрессии, проведение обучения модели на реальных данных и оценка её качество.

Постановка задачи:

1) Провести обучение модели линейной регрессии на датасете с Kaggle:

2) Загрузить датасет из репозитория (например, kaggle.com или аналогичных платформ).

3) Подготовить данные: провести первичный анализ, визуализировать распределение признаков и целевой переменной. 

4) Провести предобработку данных: удалить пропущенные значения, закодировать категориальные переменные (опционально), нормализовать признаки.

5) Построить матрицу корреляций. Сделать выводы о наличии мультиколлинеарности (расчет VIF-коэффициента).

6) Построить регрессионные модели (линейная и гребневая). Если целевая переменная - категориальная, то исследовать логистическую регрессию. Разделить на тренировочную и тестовую выборки (80/20 или 70/30). Использовать кросс-валидацию. Оценить качество построенной модели с помощью метрик: RMSE (Root Mean Square Error), R² (коэффициент детерминации) и MAPE (Mean Absolute Percentage Error).

7) Устранить мультиколлинеарность и снизить размерность признаков с помощью метода главных компонент (PCA).

8)Повторить шаг 5 (линейная и гребневая регрессия), но использовать в качестве признаков не исходные данные, а главные компоненты. Сравнить метрики качества (RMSE, R² и MAPE) моделей, обученных на исходных данных и на главных компонентах.


## Анализ данных
Для исследования использовался датасет day.csv (Bike Sharing Dataset), содержащий 731 запись и 16 признаков.  

Основные признаки:
- `season`, `yr`, `mnth`, `weekday`, `holiday`, `workingday`, `weathersit` — категориальные и булевые характеристики.  
- `temp`, `atemp`, `hum`, `windspeed` — числовые признаки.  
- `cnt` — целевая переменная (общее количество арендованных велосипедов за день).  

### Распределение целевой переменной
Гистограмма количества аренд велосипедов (cnt) показывает, что распределение имеет близкую к симметричной форму. Значение коэффициента асимметрии составило Skew = –0.047, что практически равно нулю и говорит об отсутствии выраженной смещённости распределения влево или вправо. Коэффициент эксцесса оказался равным Kurtosis = –0.815, что указывает на более приплюснутое распределение по сравнению с нормальным: центральный пик выражен слабее, а хвосты распределения несколько шире. Таким образом, целевая переменная cnt имеет распределение, близкое к нормальному, что является благоприятным фактором для построения регрессионных моделей.

### Корреляции
Корреляционная матрица выявила сильную линейную зависимость между признаками `temp` и `atemp`.  
Для формальной проверки был рассчитан VIF (Variance Inflation Factor):  
- `temp` — 63.32  
- `atemp` — 64.34  
Остальные признаки имеют VIF < 5, то есть мультиколлинеарность отсутствует.  


## Подготовка данных
1. Исключены признаки `instant`, `dteday`, `casual`, `registered`.  
2. Данные разделены на признаки `X` и целевую переменную `y`.  
3. Выполнена стандартизация признаков с помощью `StandardScaler`.  
4. Разделение на выборки: train/test = 80/20.  
5. Кросс-валидация: KFold (5 фолдов, shuffle=True, random_state=42).  


## Ход работы

### 1. Модели до и после устранения мультиколлинеарности (путем удаления atemp)
Были построены:
- Линейная регрессия 
- Гребневая регрессия 

#### Метрики:
| Модель                          | RMSE    | R²     | MAPE    |
|--------------------------------|---------|--------|---------|
| Линейная (с atemp)             | 831.29  | 0.828  | 149.38% |
| Гребневая (с atemp, α=10)      | 832.36  | 0.827  | 148.81% |
| Линейная (без atemp)           | 832.78  | 0.827  | 147.25% |
| Гребневая (без atemp, α=10)    | 834.08  | 0.827  | 147.01% |

Вывод: Результаты моделей показывают, что исключение признака `atemp` особо ничего не меняет. Линейная регрессия с `atemp` показала RMSE около 831,29 и R² 0,828, что означает, что модель объясняет примерно 82,8% вариации количества аренд. Гребневая регрессия с `atemp` немного хуже по RMSE (832,36), но немного лучше по MAPE (148,81%), что указывает на небольшое улучшение точности в процентах ошибок.

Исключение `atemp` незначительно изменяет метрики: линейная модель без `atemp` имеет RMSE 832,78, R² 0,827 и MAPE 147,25%, а гребневая регрессия — RMSE 834,08, R² 0,827 и MAPE 147,01%. Таким образом, устранение мультиколлинеарности за счёт удаления atemp не улучшает качество предсказаний, поскольку информация, которую он несёт, почти полностью дублируется другим признаком temp.
В целом, все четыре модели показывают сопоставимые результаты, с высокой точностью предсказания тренда, но относительно большой ошибкой в процентах (MAPE 147–149%), что характерно для данных с высокой дисперсией.


### 2. PCA (устранение мультиколлинеарности)
Для устранения корреляции между `temp` и `atemp` применён метод главных компонент (PCA).  

#### Метод локтя  
По кумулятивной доле объяснённой дисперсии выбрано 8 главных компонент, объясняющих 95% общей дисперсии.  

#### Метрики на PCA-признаках:
| Модель                  | RMSE    | R²     | MAPE    |
|--------------------------|---------|--------|---------|
| Линейная PCA             | 852.42  | 0.819  | 146.24% |
| Гребневая PCA (α=10)     | 854.40  | 0.818  | 146.29% |

Вывод: Применение метода главных компонент (PCA) позволило сократить число признаков до 8 при сохранении примерно 95% информации в данных. Это решило проблему мультиколлинеарности, так как главные компоненты не коррелируют друг с другом. Однако качество моделей на преобразованных признаках оказалось чуть хуже, чем на исходных данных. RMSE вырос (с 831–834 до 852–854), коэффициент детерминации R^2 немного снизился (с 0.827 до 0.819), а MAPE почти не изменился (с 147–149% до 146%). Скорее всего, это связано с тем, что PCA хоть и убирает корреляции, но при этом уменьшает интерпретируемость признаков и сглаживает важные зависимости с целевой переменной. Кроме того, мультиколлинеарность в исходных данных не сильно мешала моделям, так как гребневая регрессия частично компенсирует её за счёт регуляризации.
В итоге можно сказать, что в данной задаче использование PCA не дало преимуществ и даже немного ухудшило качество. Более разумно работать с исходными признаками, а проблему мультиколлинеарности решать с помощью регуляризации.


## Заключение
Проверка показала, что сильная корреляция наблюдается только между признаками temp и atemp. Исключение признака atemp практически не повлияло на качество моделей: все метрики (RMSE, R^2, MAPE) остались на сопоставимом уровне. Это объясняется тем, что информация, содержащаяся в atemp, дублируется temp, а гребневая регрессия уже умеет частично компенсировать мультиколлинеарность за счёт регуляризации.
Применение метода главных компонент (PCA) позволило сократить число признаков до 8 и убрать мультиколлинеарность, но качество моделей при этом немного снизилось: RMSE вырос, а R^2 уменьшился. Это связано с тем, что PCA, устраняя корреляции, одновременно теряет интерпретируемость и ослабляет полезные связи с целевой переменной.
Таким образом, можно сделать вывод, что в данной задаче устранение мультиколлинеарности с помощью удаления признака или PCA не приводит к улучшению качества прогнозов. Более целесообразным оказывается использование исходных данных вместе с методами регуляризации.


## Список источников
1. Bike Sharing Dataset: [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)  
2. Айвазян С.А. Методы эконометрики. – М.: Магистр: - ИНФРА-М, 2010. – 512 с. 
3. [Документация Scikit-learn](https://scikit-learn.ru/stable/user_guide.html)
4. [Документация Pandas](https://pandas.pydata.org/docs/)


## Приложение
Полный листинг кода приведён в файле [`lab-1.ipynb`](./lab-1.ipynb).

